{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmJR9USPnOy3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Decision Tree, and how does it work?\n",
        "\n",
        "- A Decision Tree is a supervised learning algorithm used for both classification and regression tasks.\n",
        "- It works by recursively partitioning the data into subsets based on the values of different input features.\n",
        "- It creates a tree-like structure where:\n",
        "\n",
        "1.Nodes: Represent tests on an attribute (feature).\n",
        "\n",
        "2. Branches: Represent the outcome of the test (i.e., the possible values of the attribute).\n",
        "\n",
        "3. Leaves: Represent the final decision or prediction (class label for classification, or value for regression).\n",
        "\n",
        "How it works:\n",
        "\n",
        "Start at the root node: The algorithm starts with the entire dataset at the root.\n",
        "\n",
        "Select the best attribute: It chooses the \"best\" attribute to split the data based on a criterion (e.g., maximizing information gain or minimizing impurity).\n",
        "\n",
        "Split the data: The data is split into subsets based on the values of the chosen attribute. Each subset becomes a branch of the tree.\n",
        "\n",
        "Repeat: Steps 2 and 3 are repeated recursively for each subset (branch) until a stopping condition is met. Stopping conditions can include:\n",
        "\n",
        "All data points in a subset belong to the same class (pure node).\n",
        "\n",
        "The tree reaches a maximum depth.\n",
        "\n",
        "The number of data points in a subset is below a threshold.\n",
        "\n",
        "Further splitting does not significantly improve the purity of the subsets.\n",
        "\n",
        "Assign a leaf node: When a stopping condition is met, a leaf node is created. For classification, the leaf node is assigned the most common class label in the subset. For regression, the leaf node is assigned the average value of the target variable in the subset.\n",
        "\n",
        "3.What are impurity measures in Decision Trees?\n",
        "\n",
        "- Impurity measures are used to evaluate the homogeneity of the target variable within a subset of the data.\n",
        "- A node is considered \"pure\" if all data points in it belong to the same class (in classification) or have very similar target values (in regression). Impurity measures quantify how \"mixed up\" the classes are within a node.\n",
        " Common impurity measures include:\n",
        "\n",
        "- Gini Impurity: Measures the probability of misclassifying a randomly chosen element from the subset if it were randomly labeled according to the class distribution in the subset.\n",
        "\n",
        "- Entropy: Measures the disorder or randomness of the class distribution in the subset.\n",
        "\n",
        "- Variance: Used in Regression trees to see how much the data deviates from the mean.\n",
        "\n",
        "4.What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "- For a node with n classes, let p_i be the proportion of data points belonging to class i in that node. Then, the Gini Impurity is calculated as:\n",
        "\n",
        "- Gini Impurity = 1 - Σ (p_i)^2  (summation from i=1 to n)\n",
        "\n",
        "Where:\n",
        "\n",
        "- p_i is the probability of class i in the node (i.e., the proportion of instances belonging to class i).\n",
        "\n",
        "- Σ represents the sum over all classes.\n",
        "\n",
        "5.What is the mathematical formula for Entropy?\n",
        "\n",
        "For a node with n classes, let p_i be the proportion of data points belonging to class i in that node. Then, the Entropy is calculated as:\n",
        "\n",
        "Entropy = - Σ p_i * log2(p_i)  (summation from i=1 to n)\n",
        "Where:\n",
        "\n",
        "p_i is the probability of class i in the node.\n",
        "\n",
        "log2 is the base-2 logarithm. (You can also use natural logarithm (ln or log_e) but the units of information are then in \"nats\" instead of \"bits\".)\n",
        "\n",
        "What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain (IG) measures the reduction in entropy or impurity achieved by splitting the data on a particular attribute. It's used to determine the best attribute to split on at each node of the tree. The attribute with the highest information gain is chosen.\n",
        "\n",
        "Information Gain = Entropy(parent) - Σ [ (|children_v| / |parent|) * Entropy(children_v) ]\n",
        "Where:\n",
        "\n",
        "- Entropy(parent) is the entropy of the parent node before the split.\n",
        "\n",
        "- children_v represents each child node resulting from splitting the parent node on attribute v.\n",
        "\n",
        "- |children_v| is the number of data points in child node v.\n",
        "\n",
        "- |parent| is the number of data points in the parent node.\n",
        "\n",
        "The summation is over all child nodes resulting from the split.\n",
        "\n",
        "6.What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Calculation: Gini Impurity involves squaring probabilities and subtracting from 1, while Entropy involves logarithms and probabilities.\n",
        "\n",
        "Computational Cost: Gini Impurity is generally faster to compute than Entropy because it avoids the computationally expensive logarithm operation.\n",
        "\n",
        "Sensitivity: Entropy is slightly more sensitive to changes in class probabilities than Gini Impurity. In practice, the difference in performance between the two is often minimal. Both tend to produce similar trees.\n",
        "\n",
        "Bias: Some argue that Gini is biased towards multi-valued attributes.\n",
        "\n",
        "In most cases, the choice between Gini Impurity and Entropy doesn't significantly impact the performance of the decision tree. Gini is often the default due to its lower computational cost.\n",
        "\n",
        "What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "- At its core, a decision tree is a piece-wise constant approximation of the true underlying function. It attempts to create regions in the feature space where the target variable is relatively constant.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "1. Feature Space Partitioning:\n",
        "- A decision tree recursively divides the feature space into rectangular regions.\n",
        "- Each split is based on a single feature and a threshold value. This creates a hierarchy of decisions that lead to increasingly homogeneous regions.\n",
        "\n",
        "-Optimization Problem: The goal is to find the \"best\" splits at each node. This is an optimization problem where we try to minimize the impurity of the resulting child nodes.\n",
        "- The impurity measure (Gini, Entropy, Variance) acts as the objective function.\n",
        "\n",
        "-Greedy Approach: Finding the absolute best tree structure is computationally infeasible for large datasets.\n",
        "- Therefore, decision tree algorithms typically use a greedy approach. At each node, they choose the split that provides the largest immediate reduction in impurity, without considering the long-term impact on the overall tree structure.\n",
        "\n",
        "Mathematical Representation (Classification): For a classification tree, the prediction for a new instance x can be represented as:\n",
        "\n",
        "f(x) =  c_m   if  x ∈ R_m\n",
        "\n",
        "Where:\n",
        "\n",
        "f(x) is the predicted class label for instance x.\n",
        "\n",
        "R_m is the rectangular region in the feature space corresponding to leaf node m.\n",
        "\n",
        "c_m is the most frequent class label in the training data that falls into region R_m.\n",
        "Mathematical Representation (Regression): For a regression tree, the prediction for a new instance x can be represented as:\n",
        "f(x) =  μ_m   if  x ∈ R_m\n",
        "\n",
        "Where:\n",
        "\n",
        "f(x) is the predicted value for instance x.\n",
        "\n",
        "R_m is the rectangular region in the feature space corresponding to leaf node m.\n",
        "\n",
        "μ_m is the average value of the target variable in the training data that falls into region R_m.\n",
        "\n",
        "- What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "- Pre-pruning (also called early stopping) involves setting constraints on the tree growth during the training process.\n",
        "- These constraints prevent the tree from becoming too complex and overfitting the training data. Common pre-pruning techniques include:\n",
        "\n",
        "- Maximum Depth: Limiting the maximum depth of the tree.\n",
        "\n",
        "- Minimum Samples per Leaf: Requiring a minimum number of data points in each leaf node.\n",
        "\n",
        "- Minimum Samples per Split: Requiring a minimum number of data points in a node before it can be split.\n",
        "\n",
        "- Maximum Number of Leaves: Limiting the total number of leaf nodes in the tree.\n",
        "\n",
        "Significance Test: Splitting a node only if the improvement in impurity is statistically significant.\n",
        "\n",
        "What is Post-Pruning in Decision Trees?\n",
        "\n",
        "- Post-pruning (also called backward pruning) involves growing the tree to its full extent and then pruning (removing) branches that do not significantly improve performance on a validation set or based on a complexity cost function.\n",
        "Common post-pruning techniques include:\n",
        "\n",
        "- Reduced Error Pruning: Iteratively remove subtrees and replace them with leaf nodes. The subtree is removed only if the resulting tree performs better on a validation set.\n",
        "\n",
        "- Cost Complexity Pruning (Weakest Link Pruning): Introduces a cost complexity parameter (alpha) that penalizes trees with more nodes. The algorithm finds the subtree that minimizes a cost complexity function:\n",
        "\n",
        "Cost = Error + α * Number_of_Leaves\n",
        "\n",
        "The tree is pruned by collapsing the internal node that results in the smallest increase in Cost. This process is repeated for various values of alpha. The final tree is selected based on its performance on a validation set.\n",
        "\n",
        "What is the difference between Pre-Pruning and Post-Pruning?\n",
        "Feature:\n",
        "1.Feature\n",
        "-Pre-Pruning -Applied during the tree construction process.\n",
        "- Post-Pruning-\tApplied after the tree has been fully grown.\n",
        "\n",
        "2.Method\n",
        "Pre-Pruning-Stops the tree from growing too deep/complex.\n",
        "Post-Pruning-\n",
        "Post-Pruning-Removes branches from an already grown tree.\n",
        "\n",
        "3.Data Usage:\n",
        "Pre-Pruning:Only uses the training data.\n",
        "Post-Pruning:\n",
        "Often uses a validation set in addition to training data.\n",
        "\n",
        "4.Overfitting\n",
        "Pre-Pruning:\n",
        "Prevents overfitting by limiting complexity upfront.\n",
        "\n",
        "- Post-Pruning:\n",
        "Applied after the tree has been fully grown.\n",
        "\n",
        "5.Complexity:\n",
        "-Pre-Pruning:\n",
        "Simpler to implement.\n",
        "\n",
        "-Post-Pruning:\n",
        "Can be computationally more expensive.\n",
        "\n",
        "6.Potential:\n",
        "- Pre-Pruning:\n",
        "May underfit if constraints are too strict.\n",
        "- Post-Pruning:\n",
        "Can potentially achieve better generalization.\n",
        "\n",
        "11.What is a Decision Tree Regressor?\n",
        "\n",
        "A - Decision Tree Regressor is a decision tree used for regression tasks. Instead of predicting a class label, it predicts a continuous numerical value.\n",
        "- The splitting criteria are based on minimizing variance or mean squared error (MSE) within the leaf nodes.\n",
        "- The prediction for a new instance is typically the average value of the target variable for the training instances that fall into the same leaf node.\n",
        "\n",
        "12.What are the advantages and disadvantages of Decision Trees?\n",
        "Advantages:\n",
        "\n",
        "-Easy to understand and interpret: The tree structure is intuitive and allows for easy visualization of the decision-making process.\n",
        "\n",
        "- Handles both numerical and categorical data: No need for extensive data preprocessing like one-hot encoding for numerical or categorical data.\n",
        "\n",
        "- Non-parametric: Makes no assumptions about the distribution of the data.\n",
        "\n",
        "- Feature importance: Can identify the most important features in the dataset.\n",
        "\n",
        "- Relatively fast to train and predict: Compared to some other complex algorithms.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Overfitting: Prone to overfitting the training data, especially with deep trees.\n",
        "\n",
        "- High variance: Small changes in the training data can lead to significantly different tree structures.\n",
        "\n",
        "- Bias towards features with many levels: When using Information Gain with categorical variables, features with more levels can appear to be more informative\n",
        "\n",
        "- Instability: Can be sensitive to small variations in the data.\n",
        "\n",
        "- Not suitable for highly complex relationships: Decision trees can struggle with learning complex non-linear relationships.\n",
        "\n",
        "- Greedy approach: The greedy algorithm may not find the globally optimal tree.\n",
        "\n",
        "How does a Decision Tree handle missing values?\n",
        "\n",
        "Decision trees can handle missing values in a few ways:\n",
        "\n",
        "- Ignore the missing values: In some implementations, instances with missing values are simply excluded from the splitting process.\n",
        "This is the simplest approach but can lead to information loss.\n",
        "\n",
        "- Imputation: Fill in the missing values with a plausible value, such as the mean, median, or mode of the feature.\n",
        "\n",
        "- Surrogate splits: When a node is split, the algorithm can also identify \"surrogate splits\" – splits on other features that produce similar partitions of the data.\n",
        "- If a data point has a missing value for the primary split feature, the surrogate split can be used instead. This is more sophisticated.\n",
        "\n",
        "- Treat missing values as a separate category: For categorical features, missing values can be treated as a separate category and assigned to one of the branches.\n",
        "\n",
        "- Fractional Observations: In some implementations, data points with missing values are passed down multiple branches with fractional weights based on the distribution of the non-missing values for that feature.\n",
        "\n",
        "13.How does a Decision Tree handle categorical features?\n",
        "\n",
        "-Decision Trees can handle categorical features directly, without requiring one-hot encoding or other transformations. The way it works depends on the type of categorical feature:\n",
        "\n",
        "Binary Categorical Features: For binary features (e.g., \"yes\" or \"no\"), the split is straightforward: one branch for \"yes\" and one branch for \"no\".\n",
        "\n",
        "- Multi-valued Categorical Features: For features with more than two categories (e.g., \"red\", \"green\", \"blue\"):\n",
        "\n",
        "- Multi-way Split: The algorithm can create a separate branch for each category. This can lead to a large number of branches and potential overfitting.\n",
        "\n",
        "- Binary Split: The algorithm can search for the best binary split, grouping the categories into two subsets. This is often done using an exhaustive search or a heuristic algorithm. For instance, if the categorical variable is \"color\" with options \"red\", \"blue\", and \"green\", the tree might try splitting the data into \"red\" vs. (\"blue\", \"green\") or \"blue\" vs. (\"red\", \"green\"), and so on, picking the split that optimizes the impurity measure.\n",
        "What are some real-world applications of Decision Trees\n",
        "1. Credit Risk Assessment:\n",
        "\n",
        "Application: Banks and financial institutions use decision trees to evaluate the creditworthiness of loan applicants.\n",
        "\n",
        "How it works: The tree uses factors like credit history, income, employment status, and debt-to-income ratio to classify applicants as low-risk or high-risk, helping to decide whether to approve a loan or not.\n",
        "\n",
        "2. Medical Diagnosis:\n",
        "\n",
        "Application: Decision trees can assist doctors in diagnosing diseases based on patient symptoms, medical history, and test results.\n",
        "\n",
        "How it works: The tree uses a series of questions or tests to narrow down the possible diagnoses. For example, a decision tree might help determine if a patient has a particular type of cancer based on the presence or absence of certain symptoms and biomarkers.\n",
        "\n",
        "3. Customer Relationship Management (CRM):\n",
        "\n",
        "Application: Businesses use decision trees to analyze customer behavior and predict customer churn (likelihood of leaving).\n",
        "\n",
        "How it works: The tree uses customer data like demographics, purchase history, website activity, and customer service interactions to identify customers who are at risk of churning. This allows companies to proactively offer incentives or personalized service to retain them.\n",
        "\n",
        "4. Fraud Detection:\n",
        "\n",
        "Application: Financial institutions and e-commerce companies use decision trees to detect fraudulent transactions.\n",
        "\n",
        "How it works: The tree uses transaction data like amount, location, time, and device information to identify patterns that are indicative of fraud. Transactions that are flagged as suspicious can be further investigated.\n",
        "\n",
        "5. Image Classification:\n",
        "\n",
        "Application: While deep learning models are more common for complex image classification, decision trees (or ensembles of decision trees like Random Forests) can be used for simpler image classification tasks.\n",
        "\n",
        "How it works: The tree uses pixel values or features extracted from images (e.g., edges, textures) to classify the image into different categories (e.g., cat, dog, car).\n",
        "\n",
        "6. Spam Filtering:\n",
        "\n",
        "Application: Email providers use decision trees to filter out spam emails.\n",
        "\n",
        "How it works: The tree uses features like the sender's address, the subject line, the content of the email, and the presence of certain keywords to classify emails as spam or not spam.\n",
        "\n",
        "7. Marketing:\n",
        "\n",
        "Application: Businesses use decision trees to segment customers and target them with personalized marketing campaigns.\n",
        "\n",
        "How it works: The tree uses customer data like demographics, purchase history, and online behavior to identify different customer segments. Each segment can then be targeted with specific marketing messages and offers.\n",
        "\n",
        "8. Process Optimization:\n",
        "\n",
        "Application: Manufacturing companies use decision trees to optimize their production processes.\n",
        "\n",
        "How it works: The tree uses data on machine performance, materials, and environmental conditions to identify factors that are affecting production efficiency and quality.\n",
        "\n",
        "9. Risk Management:\n",
        "\n",
        "Application: Insurance companies use decision trees to assess risk associated with insuring individuals or assets.\n",
        "\n",
        "How it works: The tree uses factors like age, health, lifestyle, location, and property characteristics to estimate the likelihood of a claim.\n",
        "\n",
        "10. Recommender Systems:\n",
        "\n",
        "Application: Decision trees can be used as part of recommender systems to suggest products or services to users.\n",
        "\n",
        "How it works: The tree uses user data like past purchases, browsing history, and demographics to predict what items the user might be interested in.\n",
        "\n",
        "Key Reasons for Use:\n",
        "\n",
        "- Interpretability: Decision trees are easily understandable by non-technical stakeholders, making them valuable for explaining decisions.\n",
        "\n",
        "- Feature Importance: They provide insights into which features are most influential in making predictions.\n",
        "\n",
        "- Versatility: They can handle both categorical and numerical data and can be used for classification and regression tasks.\n",
        "\n",
        "- Speed: They are relatively fast to train and make predictions, making them suitable for real-time applications. (However, more complex ensembles like Random Forests can be slower).\n",
        "\n",
        "1. Credit Risk Assessment:\n",
        "\n",
        "Application: Banks and financial institutions use decision trees to evaluate the creditworthiness of loan applicants.\n",
        "\n",
        "How it works: The tree uses factors like credit history, income, employment status, and debt-to-income ratio to classify applicants as low-risk or high-risk, helping to decide whether to approve a loan or not.\n",
        "\n",
        "2. Medical Diagnosis:\n",
        "\n",
        "Application: Decision trees can assist doctors in diagnosing diseases based on patient symptoms, medical history, and test results.\n",
        "\n",
        "How it works: The tree uses a series of questions or tests to narrow down the possible diagnoses. For example, a decision tree might help determine if a patient has a particular type of cancer based on the presence or absence of certain symptoms and biomarkers.\n",
        "\n",
        "3. Customer Relationship Management (CRM):\n",
        "\n",
        "Application: Businesses use decision trees to analyze customer behavior and predict customer churn (likelihood of leaving).\n",
        "\n",
        "How it works: The tree uses customer data like demographics, purchase history, website activity, and customer service interactions to identify customers who are at risk of churning. This allows companies to proactively offer incentives or personalized service to retain them.\n",
        "\n",
        "4. Fraud Detection:\n",
        "\n",
        "Application: Financial institutions and e-commerce companies use decision trees to detect fraudulent transactions.\n",
        "\n",
        "How it works: The tree uses transaction data like amount, location, time, and device information to identify patterns that are indicative of fraud. Transactions that are flagged as suspicious can be further investigated.\n",
        "\n",
        "5. Image Classification:\n",
        "\n",
        "Application: While deep learning models are more common for complex image classification, decision trees (or ensembles of decision trees like Random Forests) can be used for simpler image classification tasks.\n",
        "\n",
        "How it works: The tree uses pixel values or features extracted from images (e.g., edges, textures) to classify the image into different categories (e.g., cat, dog, car).\n",
        "\n",
        "6. Spam Filtering:\n",
        "\n",
        "Application: Email providers use decision trees to filter out spam emails.\n",
        "\n",
        "How it works: The tree uses features like the sender's address, the subject line, the content of the email, and the presence of certain keywords to classify emails as spam or not spam.\n",
        "\n",
        "7. Marketing:\n",
        "\n",
        "Application: Businesses use decision trees to segment customers and target them with personalized marketing campaigns.\n",
        "\n",
        "How it works: The tree uses customer data like demographics, purchase history, and online behavior to identify different customer segments. Each segment can then be targeted with specific marketing messages and offers.\n",
        "\n",
        "8. Process Optimization:\n",
        "\n",
        "Application: Manufacturing companies use decision trees to optimize their production processes.\n",
        "\n",
        "How it works: The tree uses data on machine performance, materials, and environmental conditions to identify factors that are affecting production efficiency and quality.\n",
        "\n",
        "9. Risk Management:\n",
        "\n",
        "Application: Insurance companies use decision trees to assess risk associated with insuring individuals or assets.\n",
        "\n",
        "How it works: The tree uses factors like age, health, lifestyle, location, and property characteristics to estimate the likelihood of a claim.\n",
        "\n",
        "10. Recommender Systems:\n",
        "\n",
        "Application: Decision trees can be used as part of recommender systems to suggest products or services to users.\n",
        "\n",
        "How it works: The tree uses user data like past purchases, browsing history, and demographics to predict what items the user might be interested in.\n",
        "\n",
        "Key Reasons for Use:\n",
        "\n",
        "Interpretability: Decision trees are easily understandable by non-technical stakeholders, making them valuable for explaining decisions.\n",
        "\n",
        "Feature Importance: They provide insights into which features are most influential in making predictions.\n",
        "\n",
        "Versatility: They can handle both categorical and numerical data and can be used for classification and regression tasks.\n",
        "\n",
        "Speed: They are relatively fast to train and make predictions, making them suitable for real-time applications. (However, more complex ensembles like Random Forests can be slower).\n",
        "\n"
      ],
      "metadata": {
        "id": "QkN6E7ZJnlFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import graphviz\n",
        "def train_iris_gini():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    clf = DecisionTreeClassifier(random_state=42)  # Gini impurity by default\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Iris Decision Tree (Gini) Accuracy:\", accuracy)\n",
        "train_iris_gini()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eck7F587pMWw",
        "outputId": "f0c63860-7bf8-4999-d145-0cb60db0b833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Decision Tree (Gini) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances*\n",
        "def train_iris_gini_feature_importance():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    clf = DecisionTreeClassifier(random_state=42)  # Gini impurity by default\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Iris Decision Tree (Gini) Feature Importances:\", clf.feature_importances_)\n",
        "train_iris_gini_feature_importance()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctfUAmcK5OQS",
        "outputId": "a91be547-890b-4f92-c841-a1c1e70a5f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Decision Tree (Gini) Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#3. * Write a Python program to train a Train Decision Tree Classifier with Entropy and print accuracy\n",
        "def train_iris_entropy():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Iris Decision Tree (Entropy) Accuracy:\", accuracy)\n",
        "train_iris_entropy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOLv3hiFsmjQ",
        "outputId": "c732fe74-273d-4fa1-dedb-0f5fb9896092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Decision Tree (Entropy) Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)*\n",
        "def train_housing_regressor():\n",
        "    housing = fetch_california_housing()\n",
        "    X = housing.data\n",
        "    y = housing.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    reg = DecisionTreeRegressor(random_state=42)\n",
        "    reg.fit(X_train, y_train)\n",
        "    y_pred = reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(\"Housing Decision Tree Regressor MSE:\", mse)\n",
        "train_housing_regressor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdoRqgFqtKNv",
        "outputId": "34726a37-7328-44bb-feab-0c4615675279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Housing Decision Tree Regressor MSE: 0.5280096503174904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program Train Decision Tree Classifier and visualize the tree using graphviz\n",
        "def train_iris_visualize_tree():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    clf = DecisionTreeClassifier(random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    dot_data = export_graphviz(clf, out_file=None,\n",
        "                                feature_names=iris.feature_names,\n",
        "                                class_names=iris.target_names,\n",
        "                                filled=True, rounded=True,\n",
        "                                special_characters=True)\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    graph.render(\"iris_decision_tree\") # saves the tree as iris_decision_tree.pdf (or .png, etc.)\n",
        "    print(\"Decision tree visualization saved to iris_decision_tree.pdf\")\n",
        "train_iris_visualize_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIlFStKfu73K",
        "outputId": "367cce4b-72ba-456d-e474-4edf477276e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree visualization saved to iris_decision_tree.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #6. Train Decision Tree Classifier with max_depth=3 and compare with a fully grown tree\n",
        "def train_iris_compare_depth():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Fully grown tree\n",
        "    clf_full = DecisionTreeClassifier(random_state=42)\n",
        "    clf_full.fit(X_train, y_train)\n",
        "    y_pred_full = clf_full.predict(X_test)\n",
        "    accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "    print(\"Iris Decision Tree (Full Depth) Accuracy:\", accuracy_full)\n",
        "\n",
        "    # Tree with max_depth=3\n",
        "    clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "    clf_depth3.fit(X_train, y_train)\n",
        "    y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "    accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "    print(\"Iris Decision Tree (Max Depth 3) Accuracy:\", accuracy_depth3)\n",
        "\n",
        "  train_iris_compare_depth()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aR1fUkf9ao3",
        "outputId": "5c3740f4-bbe3-47fa-9c5c-3a38fdc7a2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Decision Tree (Full Depth) Accuracy: 1.0\n",
            "Iris Decision Tree (Max Depth 3) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train Decision Tree Classifier with min_samples_split=5 and compare with a default tree\n",
        "def train_iris_compare_min_samples_split():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Default tree\n",
        "    clf_default = DecisionTreeClassifier(random_state=42)\n",
        "    clf_default.fit(X_train, y_train)\n",
        "    y_pred_default = clf_default.predict(X_test)\n",
        "    accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "    print(\"Iris Decision Tree (Default) Accuracy:\", accuracy_default)\n",
        "\n",
        "\n",
        "train_iris_compare_min_samples_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUjoOetzA5zZ",
        "outputId": "4b548cf3-006b-42de-ad15-aae6fe1f9c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Decision Tree (Default) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Apply feature scaling before training a Decision Tree Classifier and compare with unscaled data\n",
        "def train_iris_compare_scaling():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Unscaled data\n",
        "    clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "    clf_unscaled.fit(X_train, y_train)\n",
        "    y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "    accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "    print(\"Iris Decision Tree (Unscaled) Accuracy:\", accuracy_unscaled)\n",
        "\n",
        "    # Scaled data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "    clf_scaled.fit(X_train_scaled, y_train)\n",
        "    y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "    accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "    print(\"Iris Decision Tree (Scaled) Accuracy:\", accuracy_scaled)\n",
        "train_iris_compare_scaling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuL4kg3ECUjz",
        "outputId": "e87b9371-b977-48df-ecb0-c8a1fb203369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Decision Tree (Unscaled) Accuracy: 1.0\n",
            "Iris Decision Tree (Scaled) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-vs-Rest (OvR) Decision Tree Classifier for Multiclass Classification\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic multiclass dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=8,\n",
        "                           n_redundant=2, n_classes=3, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Wrap the Decision Tree Classifier with OneVsRestClassifier\n",
        "ovr_classifier = OneVsRestClassifier(dt_classifier)\n",
        "\n",
        "# Train the One-vs-Rest Classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn2Ha7JF_dGs",
        "outputId": "ef6ac38f-1a3f-46d5-b8e2-1ed27c669d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i+1}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tudf4SMOE_B0",
        "outputId": "eb1c4cda-3ff0-46c5-c2cc-e01816ca49f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "Feature 1: 0.2218\n",
            "Feature 2: 0.0211\n",
            "Feature 3: 0.0082\n",
            "Feature 4: 0.1048\n",
            "Feature 5: 0.2503\n",
            "Feature 6: 0.1242\n",
            "Feature 7: 0.0253\n",
            "Feature 8: 0.0097\n",
            "Feature 9: 0.0213\n",
            "Feature 10: 0.2132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree*\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate some synthetic regression data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 3 * X.squeeze() + 1.5 * np.random.randn(100) # Add some noise\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "dt_regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "\n",
        "# Create an unrestricted Decision Tree Regressor\n",
        "dt_regressor_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressors\n",
        "dt_regressor_limited.fit(X_train, y_train)\n",
        "dt_regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_limited = dt_regressor_limited.predict(X_test)\n",
        "y_pred_unrestricted = dt_regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Evaluate the performance (Mean Squared Error)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "print(f\"Mean Squared Error (max_depth=5): {mse_limited:.4f}\")\n",
        "print(f\"Mean Squared Error (unrestricted): {mse_unrestricted:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPp80CEw21JP",
        "outputId": "5717c344-860a-4244-83cc-dd12b36011a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (max_depth=5): 1.0712\n",
            "Mean Squared Error (unrestricted): 2.0347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ',* Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy*\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Get the cost complexity pruning path\n",
        "path = dt_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Train a Decision Tree for each alpha value\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Remove the last element in clfs and ccp_alphas, as it's often a trivial tree.\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Evaluate the accuracy for each tree\n",
        "train_scores = [clf.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "DToi1daBGvTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#* Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,Recall, and F1-Score*\n",
        "def train_and_evaluate_decision_tree(data, target):\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree Classifier, evaluates its performance using precision,\n",
        "    recall, and F1-score.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame or numpy.ndarray): The feature data.\n",
        "        target (pd.Series or numpy.ndarray): The target variable.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the trained model and a dictionary of metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Create a Decision Tree Classifier\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
        "\n",
        "    # Store metrics in a dictionary\n",
        "    metrics = {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1\n",
        "    }\n",
        "\n",
        "    return dt_classifier, metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "chYxTyrVNicX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,Recall, and F1-Score*\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset (replace with your own data if needed)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "bX4mVtlyR2oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset (replace with your own data if needed)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "nGyPChldUEcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sn3CPi6qndcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TUroxrGAnbc5"
      }
    }
  ]
}