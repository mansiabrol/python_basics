{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Can we use Bagging for regression problems?\n",
        "\n",
        " - Yes. Bagging (Bootstrap Aggregating) is a general ensemble technique applicable to both classification and regression problems.\n",
        " - For regression, instead of taking a majority vote (like in classification), the predictions from the individual base regressors (e.g., Decision Trees) are typically averaged to produce the final ensemble prediction.\n",
        " 2.What is the difference between multiple model training and single model training?\n",
        "- Single Model Training: Involves training one individual model (e.g., a single Decision Tree, a single Logistic Regression model, a single SVM) on the entire training dataset.\n",
        "- The goal is to find the best parameters for that one model to capture the patterns in the data. Its performance relies solely on the capability and suitability of that specific model type for the given data.\n",
        "- Multiple Model Training (Ensemble Methods): Involves training multiple models, often of the same type (homogeneous ensemble) but sometimes different types (heterogeneous ensemble).\n",
        "- These models are typically trained on different subsets of the data or with different configurations. Their individual predictions are then combined (e.g., by voting or averaging) to produce a final prediction. The idea is that the combined wisdom of multiple models is often better (more accurate, more robust) than any single model.\n",
        "\n",
        "3.Explain the concept of feature randomness in Random Forest\n",
        "\n",
        "- Random Forest builds upon Bagging (which uses bootstrap sampling of data).\n",
        "-  In addition to sampling data points for each tree, Random Forest introduces randomness in feature selection during the tree building process.\n",
        "- How it works: When splitting a node in a decision tree within the Random Forest, the algorithm doesn't consider all available features to find the best split. Instead, it randomly selects a subset of features (e.g., sqrt(n_features) for classification, n_features / 3 for regression are common defaults).\n",
        "- Only the features in this random subset are evaluated to find the best split point for that node.\n",
        "- Purpose: This feature randomness further decorrelates the trees in the forest. If one or a few features are very strong predictors, Bagging alone might still produce similar trees because those features would likely be selected near the root of most trees. By restricting the features available at each split, Random Forest forces trees to explore other, potentially less obvious, predictive features, leading to greater diversity among the trees and often better generalization (reduced variance).\n",
        "\n",
        "4.What is OOB (Out-of-Bag) Score?\n",
        "- The Out-of-Bag (OOB) score is a method for evaluating the performance of Bagging-based models (like Random Forest and BaggingClassifier/Regressor) without needing a separate validation set.\n",
        "- How it works: Because Bagging uses bootstrap sampling, each base model (e.g., each tree in a Random Forest) is trained on only a subset (roughly 63.2%) of the original training data. The remaining data points (roughly 36.8%), which were not included in the bootstrap sample for a specific tree, are called the \"Out-of-Bag\" samples for that tree.\n",
        "\n",
        "- To calculate the OOB score for the entire ensemble:\n",
        "For each data point in the original training set, identify all the trees that did not use this data point during their training (i.e., the trees for which this point was OOB).\n",
        "- Make predictions for this data point using only those specific OOB trees (e.g., by majority vote or averaging).\n",
        "Compare this prediction to the actual target value for the data point.\n",
        "- Calculate an overall metric (like accuracy for classification or R-squared/MSE for regression) based on these OOB predictions across all training data points.\n",
        "- Benefit: The OOB score provides an unbiased estimate of the model's performance on unseen data, similar to cross-validation, but obtained \"for free\" during the training process.\n",
        "\n",
        "5.How can you measure the importance of features in a Random Forest model?\n",
        "- Random Forests offer built-in mechanisms to estimate feature importance, indicating how much each feature contributes to the model's predictive power. Common methods include:\n",
        "- Mean Decrease in Impurity (MDI) / Gini Importance:\n",
        "- Calculated during training.\n",
        "- For each feature, it measures the total reduction in the node impurity criterion (like Gini impurity for classification or variance reduction/MSE for regression) averaged over all trees in the forest whenever that feature is used to split a node.\n",
        "- Features that result in larger impurity decreases are considered more important.\n",
        "- Pros: Fast to compute (available directly after training).\n",
        "- Cons: Can be biased towards high-cardinality features (features with many unique values) and might overestimate the importance of correlated features.\n",
        "- Mean Decrease in Accuracy (MDA) / Permutation Importance:\n",
        "Calculated after training, often on a validation set or the OOB samples.\n",
        "For a specific feature:\n",
        "- Measure the model's baseline accuracy (or other metric) on the dataset.\n",
        "Randomly shuffle (permute) the values of only that feature in the dataset, breaking its relationship with the target variable.\n",
        "\n",
        "6.Explain the working principle of a Bagging Classifier\n",
        "- A Bagging Classifier aims to improve the stability and accuracy of a base classification algorithm (like a Decision Tree, SVM, etc.) by reducing variance.\n",
        "Steps:\n",
        "- Bootstrap Sampling: Create multiple (say, B) different training datasets from the original training data.\n",
        "Each dataset is created by sampling with replacement from the original data. Each bootstrap sample typically has the same size as the original dataset but contains\n",
        "duplicate instances and omits some original instances (the OOB samples).\n",
        "Base Model Training: Train one instance of the base classifier (e.g., a Decision Tree) independently on each of the B bootstrap samples. This results in B different classifiers.\n",
        "- Aggregation (Voting): To make a prediction for a new, unseen data point, pass it through all B trained classifiers. Collect the individual predictions from each classifier.\n",
        "- The final prediction of the Bagging Classifier is determined by majority voting among the B individual predictions. (e.g., if 7 out of 10 trees predict 'Class A' and 3 predict 'Class B', the final prediction is 'Class A'). For probabilistic predictions, the probabilities from each base model can be averaged.\n",
        "\n",
        "\n",
        "- Measure the model's accuracy again on this permuted dataset.\n",
        "The decrease in accuracy caused by shuffling the feature indicates its importance. A larger drop means the feature was more important.\n",
        "\n",
        "- Pros: Less biased by feature cardinality, captures interactions better, can be calculated on any fitted model.\n",
        "Cons: Computationally more expensive, especially with many features or large datasets.\n",
        "\n",
        "7.How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "- You evaluate a Bagging Classifier using standard classification metrics, typically calculated on a hold-out test set (data not used during training at all). Common metrics include:\n",
        "- Accuracy: Overall percentage of correct predictions.\n",
        "Precision: Of the instances predicted as positive, how many actually are positive (TP / (TP + FP)). Important when False Positives are costly.\n",
        "- Recall (Sensitivity): Of the actual positive instances, how many were correctly identified (TP / (TP + FN)). Important when False Negatives are costly.\n",
        "- F1-Score: The harmonic mean of Precision and Recall (2 * (Precision * Recall) / (Precision + Recall)). Useful for imbalanced datasets.\n",
        "- AUC (Area Under the ROC Curve): Measures the model's ability to distinguish between positive and negative classes across different probability thresholds.\n",
        "- Confusion Matrix: A table showing True Positives, True Negatives, False Positives, and False Negatives, providing a detailed breakdown of performance.\n",
        "-Additionally, as mentioned before, the OOB Score can be used as an internal estimate of performance without needing a separate test set during development or hyperparameter tuning.\n",
        "8.How does a Bagging Regressor work?\n",
        "- Similar to the Bagging Classifier, but adapted for regression tasks.\n",
        "Steps:\n",
        "-Bootstrap Sampling: Create B bootstrap samples from the original training data (with replacement).\n",
        "-Base Model Training: Train one instance of the base regressor (e.g., a -Decision Tree Regressor) independently on each of the B bootstrap samples.\n",
        "-Aggregation (Averaging): To make a prediction for a new data point, pass it through all B trained regressors.\n",
        "- Collect the individual numerical predictions. The final prediction of the Bagging Regressor is the average (mean) of the B individual predictions.\n",
        "\n",
        "9.What is the main advantage of ensemble techniques?\n",
        "-The primary advantage is generally improved predictive performance (accuracy for classification, lower error for regression) and increased robustness compared to a single model. They achieve this by:\n",
        "-Reducing Variance (Bagging, Random Forests): Averaging predictions from diverse models trained on different data subsets smooths out noise and reduces sensitivity to the specific training data.\n",
        "-Reducing Bias (Boosting): Sequentially focusing on misclassified examples helps the model learn complex patterns it might otherwise miss.\n",
        "-Combining the strengths of different models (Stacking)\n",
        "\n",
        "10.What is the main advantage of ensemble techniques?\n",
        "- The primary advantage is generally improved predictive performance (accuracy for classification, lower error for regression) and increased robustness compared to a single model. They achieve this by:\n",
        "- Reducing Variance (Bagging, Random Forests): Averaging predictions from diverse models trained on different data subsets smooths out noise and reduces sensitivity to the specific training data.\n",
        "- Reducing Bias (Boosting): Sequentially focusing on misclassified examples helps the model learn complex patterns it might otherwise miss.\n",
        "Combining the strengths of different models (Stacking)\n",
        "\n",
        "11.What is the main advantage of ensemble techniques?\n",
        "- The primary advantage is generally improved predictive performance (accuracy for classification, lower error for regression) and increased robustness compared to a single model. They achieve this by:\n",
        "-Reducing Variance (Bagging, Random Forests): Averaging predictions from diverse models trained on different data subsets smooths out noise and reduces sensitivity to the specific training data.\n",
        "-Reducing Bias (Boosting): Sequentially focusing on misclassified examples helps the model learn complex patterns it might otherwise miss.\n",
        "Combining the strengths of different models (Stacking)\n",
        "\n",
        "12.What is a Random Forest Classifier?\n",
        "A Random Forest Classifier is a specific type of ensemble learning method primarily used for classification tasks. It operates by constructing a multitude (a \"forest\") of Decision Trees during training time.\n",
        "Key Characteristics:\n",
        "It uses Bagging (bootstrap sampling of data) to train each tree on a different random subset of the training data.\n",
        "It introduces feature randomness: at each node split during tree construction, only a random subset of features is considered for finding the best split.\n",
        "The final prediction for a new instance is obtained by taking the majority vote of the predictions from all individual trees in the forest.\n",
        "It's known for its good performance \"out-of-the-box\", robustness to overfitting, and ability to handle high-dimensional data.\n",
        "\n",
        "13.What are the main types of ensemble techniques?\n",
        "The three most common types are:\n",
        "- Bagging (Bootstrap Aggregating): Trains multiple base models independently in parallel on different bootstrap samples of the data. Combines predictions by voting (classification) or averaging (regression).\n",
        "- Aims primarily to reduce variance. (e.g., Random Forest).\n",
        "Boosting: Trains base models sequentially. Each new model focuses on correcting the errors made by the previous models (e.g., by up-weighting misclassified samples).\n",
        "-Combines predictions through a weighted vote or sum. Aims primarily to reduce bias. (e.g., AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost).\n",
        "-Stacking (Stacked Generalization): Trains multiple different base models (often diverse types).\n",
        "-Then, trains a final \"meta-model\" whose input features are the predictions made by the base models on the training data (often using cross-validation predictions to prevent leakage). The meta-model learns how to best combine the base model predictions.\n",
        "What is ensemble learning in machine learning?\n",
        "Ensemble learning is a machine learning paradigm where multiple learning algorithms (base models) are strategically combined to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Instead of relying on a single model, ensemble methods leverage the collective intelligence of several models.\n",
        "14.When should we avoid using ensemble methods?\n",
        "-Need for High Interpretability: If explaining precisely how a prediction is made is crucial (e.g., in some legal or medical contexts), the complexity of ensembles can be a drawback compared to simpler models like linear regression or single decision trees.\n",
        "Strict Computational Constraints: If training time, prediction latency, or memory usage is severely limited, the overhead of training and running multiple models might be prohibitive.\n",
        "- Very Small Datasets: With extremely limited data, creating diverse bootstrap samples for Bagging might be difficult, and the complexity of ensembles might lead to overfitting despite their variance-reducing nature. A simpler model might generalize better.\n",
        "When a Single Model Performs Exceptionally Well: If a well-tuned single model already achieves the desired performance and generalization, the added complexity and computational cost of an ensemble might not be justified.\n",
        "-Real-time Prediction Requirements: While many ensembles are fast, some (especially large ones or complex boosting models) might introduce latency unacceptable for hard real-time systems\n",
        "\n",
        "16.How does Bagging help in reducing overfitting?\n",
        "- Overfitting occurs when a model learns the training data too well, including its noise and specific idiosyncrasies, leading to poor performance on unseen data. Single complex models like deep Decision Trees are prone to overfitting.\n",
        "-Bagging reduces overfitting primarily by reducing variance:\n",
        "Diverse Training Sets: Each base model is trained on a slightly different subset of the data due to bootstrap sampling.\n",
        "-Averaging/Voting: When predictions from these diverse models are combined (averaged for regression, voted for classification), the individual errors and overfitting tendencies tend to cancel each other out. A single tree might latch onto noise in its specific bootstrap sample, but it's unlikely that all trees will overfit the same noise patterns in the same way. The averaging/voting process smooths out these individual model eccentricities, resulting in a more stable and generalizable final prediction.\n",
        "17.Why is Random Forest better than a single Decision Tree?\n",
        "Random Forest generally outperforms a single Decision Tree for several reasons:\n",
        "-Reduced Overfitting: As explained above, the combination of Bagging and feature randomness significantly reduces the variance and makes the model less prone to overfitting compared to a single, potentially deep, Decision Tree.\n",
        "-Increased Robustness: The ensemble nature makes Random Forest less sensitive to noise in the data and variations in the training set. Small changes in the data are less likely to drastically alter the final model.\n",
        "Improved Accuracy: By averaging the predictions of many decorrelated trees, Random Forest often achieves higher accuracy than a single optimized Decision Tree. The ensemble can capture more complex patterns without overfitting as severely.\n",
        "-No Need for Pruning (typically): Single Decision Trees often require careful pruning to prevent overfitting. Random Forests are less sensitive to the depth of individual trees because the ensemble averaging combats overfitting.\n",
        "\n",
        "What are some real-world applications of ensemble techniques?\n",
        "Ensemble methods are widely used across various domains due to their high performance:\n",
        "Finance: Credit scoring, fraud detection, stock market prediction.\n",
        "Healthcare: Disease diagnosis (e.g., cancer detection from images), predicting patient outcomes, drug discovery.\n",
        "E-commerce: Recommendation systems, customer churn prediction, sentiment analysis of reviews.\n",
        "Image Recognition & Computer Vision: Object detection, image classification.\n",
        "Remote Sensing: Land cover classification, environmental monitoring.\n",
        "Bioinformatics: Gene expression analysis, protein structure prediction.\n",
        "Machine Learning Competitions: Often the winning solutions on platforms like Kaggle involve sophisticated ensembles (especially Gradient Boosting and Stacking).\n",
        "What is the difference between Bagging and Boosting?\n",
        "-Feature\tBagging (e.g., Random Forest)\tBoosting (e.g., AdaBoost, GBM)\n",
        "Model Training\tParallel (Independent)\tSequential (Dependent)\n",
        "Goal\tDecrease Variance\tDecrease Bias (and often Variance too)\n",
        "Data Sampling\tBootstrap sampling (random subsets with rep.)\tOften uses all data, but weights samples\n",
        "-Model Weighting\tTypically equal weight (voting/averaging)\tWeighted (models performing better get higher weight)\n",
        "-Focus\tCreate diverse models on different data views\tFocus on correcting errors of prior models\n",
        "-Sensitivity\tLess sensitive to outliers\tCan be sensitive to outliers / noisy data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ip6E_oayzEza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "# Set random state for reproducibility\n",
        "random_seed = 42\n",
        "print(\"\\n--- 1. Bagging Classifier with Decision Trees ---\")\n",
        "# Generate sample classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=random_seed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Define the base estimator\n",
        "base_dt = DecisionTreeClassifier(random_state=random_seed)\n",
        "\n",
        "# Create and train the Bagging Classifier\n",
        "# n_estimators: number of trees\n",
        "bagging_clf = BaggingClassifier(estimator=base_dt, n_estimators=50,\n",
        "                                random_state=random_seed, n_jobs=-1) # n_jobs=-1 uses all CPU cores\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier (Decision Tree base) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs_hBCg7urBd",
        "outputId": "e228d2e1-093f-4acd-d6d6-91249fa3c09d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. Bagging Classifier with Decision Trees ---\n",
            "Bagging Classifier (Decision Tree base) Accuracy: 0.8733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# . Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "print(\"\\n--- 2. Bagging Regressor with Decision Trees ---\")\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15,\n",
        "                       noise=0.1, random_state=random_seed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Define the base estimator\n",
        "base_dtr = DecisionTreeRegressor(random_state=random_seed)\n",
        "\n",
        "# Create and train the Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(estimator=base_dtr, n_estimators=50,\n",
        "                               random_state=random_seed, n_jobs=-1)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor (Decision Tree base) MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh0tCLUbvUqZ",
        "outputId": "8c8b83f5-3bf3-42e2-df25-7413b4c514ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Bagging Regressor with Decision Trees ---\n",
            "Bagging Regressor (Decision Tree base) MSE: 17888.1752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "print(\"\\n--- 3. Random Forest Classifier & Feature Importance ---\")\n",
        "# Load Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy (optional, but good practice)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy (Breast Cancer): {accuracy:.4f}\")\n",
        "\n",
        "# Get feature importances (Mean Decrease in Impurity)\n",
        "importances = rf_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1] # Sort features by importance\n",
        "\n",
        "print(\"\\nFeature Importance Scores (MDI):\")\n",
        "for i in range(X.shape[1]):\n",
        "    print(f\"{i + 1}. Feature '{feature_names[indices[i]]}' ({importances[indices[i]]:.4f})\")"
      ],
      "metadata": {
        "id": "YVrlSYBfxTc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "print(\"\\n--- 4. Random Forest Regressor vs Single Decision Tree ---\")\n",
        "# Generate sample regression data (can reuse from example 2)\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15,\n",
        "                       noise=0.1, random_state=random_seed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# --- Single Decision Tree Regressor ---\n",
        "single_dtr = DecisionTreeRegressor(random_state=random_seed)\n",
        "single_dtr.fit(X_train, y_train)\n",
        "y_pred_single = single_dtr.predict(X_test)\n",
        "mse_single = mean_squared_error(y_test, y_pred_single)\n",
        "print(f\"Single Decision Tree Regressor MSE: {mse_single:.4f}\")\n",
        "\n",
        "# --- Random Forest Regressor ---\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "if mse_rf < mse_single:\n",
        "    print(\"Random Forest Regressor performed better (lower MSE).\")\n",
        "else:\n",
        "    print(\"Single Decision Tree Regressor performed better or equal (lower/equal MSE).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbux64O5yU45",
        "outputId": "84f2c6b1-6018-4b93-c605-6e59f9d9e935"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 4. Random Forest Regressor vs Single Decision Tree ---\n",
            "Single Decision Tree Regressor MSE: 42008.5183\n",
            "Random Forest Regressor MSE: 17137.6945\n",
            "Random Forest Regressor performed better (lower MSE).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "print(\"\\n--- 5. Random Forest Classifier OOB Score ---\")\n",
        "# Use Breast Cancer data from example 3\n",
        "X, y = load_breast_cancer().data, load_breast_cancer().target\n",
        "\n",
        "# Split data (although OOB is calculated on training data, we split for consistency)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Create and train RF Classifier WITH OOB_SCORE=TRUE\n",
        "# Note: Needs enough estimators for OOB score to be reliable (e.g., >= 10)\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=100, random_state=random_seed,\n",
        "                                    oob_score=True, # <<< Enable OOB score calculation\n",
        "                                    n_jobs=-1)\n",
        "rf_clf_oob.fit(X_train, y_train) # OOB score calculated during fit\n",
        "\n",
        "# Access the OOB score\n",
        "oob_accuracy = rf_clf_oob.oob_score_\n",
        "print(f\"Random Forest Classifier OOB Accuracy: {oob_accuracy:.4f}\")\n",
        "\n",
        "# Compare with test set accuracy (optional)\n",
        "y_pred_test = rf_clf_oob.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"Random Forest Classifier Test Set Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "1vbwrpzeyol2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "print(\"\\n--- 6. Bagging Classifier with SVM Base ---\")\n",
        "# Generate sample classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=random_seed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Define the base estimator (SVM Classifier)\n",
        "# Note: SVM can be slow, especially for Bagging with many estimators/large data\n",
        "# Using a smaller n_estimators here for speed\n",
        "base_svm = SVC(probability=True, random_state=random_seed) # probability=True often helps ensembles\n",
        "\n",
        "# Create and train the Bagging Classifier\n",
        "bagging_clf_svm = BaggingClassifier(estimator=base_svm, n_estimators=10, # Fewer estimators due to SVM speed\n",
        "                                    random_state=random_seed, n_jobs=-1)\n",
        "bagging_clf_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf_svm.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier (SVM base) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "lQcEkbtxzf__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "print(\"\\n--- 7. Random Forest Classifier Accuracy vs Number of Trees ---\")\n",
        "# Use Breast Cancer data\n",
        "X, y = load_breast_cancer().data, load_breast_cancer().target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "accuracies = []\n",
        "\n",
        "print(\"Comparing RF accuracy for different n_estimators:\")\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_seed, n_jobs=-1)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"  n_estimators = {n_estimators}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Plot the results\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.plot(n_estimators_list, accuracies, marker='o')\n",
        "# plt.title(\"Random Forest Accuracy vs. Number of Trees (Breast Cancer)\")\n",
        "# plt.xlabel(\"Number of Estimators (Trees)\")\n",
        "# plt.ylabel(\"Test Set Accuracy\")\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "7X1TCd1jz9nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "print(\"\\n--- 8. Bagging Classifier with Logistic Regression Base (AUC) ---\")\n",
        "# Generate sample classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=random_seed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Define the base estimator (Logistic Regression)\n",
        "base_lr = LogisticRegression(solver='liblinear', random_state=random_seed) # liblinear good for smaller datasets\n",
        "\n",
        "# Create and train the Bagging Classifier\n",
        "bagging_clf_lr = BaggingClassifier(estimator=base_lr, n_estimators=50,\n",
        "                                   random_state=random_seed, n_jobs=-1)\n",
        "bagging_clf_lr.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions for AUC\n",
        "y_pred_proba = bagging_clf_lr.predict_proba(X_test)[:, 1] # Probability of positive class\n",
        "\n",
        "# Evaluate AUC\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Bagging Classifier (Logistic Regression base) AUC: {auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ4jBW_w1uSv",
        "outputId": "8ce01855-a17f-4779-984a-7b25e4d8f33d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 8. Bagging Classifier with Logistic Regression Base (AUC) ---\n",
            "Bagging Classifier (Logistic Regression base) AUC: 0.9075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "print(\"\\n--- 8. Bagging Classifier with Logistic Regression Base (AUC) ---\")\n",
        "# Generate sample classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=random_seed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Define the base estimator (Logistic Regression)\n",
        "base_lr = LogisticRegression(solver='liblinear', random_state=random_seed) # liblinear good for smaller datasets\n",
        "\n",
        "# Create and train the Bagging Classifier\n",
        "bagging_clf_lr = BaggingClassifier(estimator=base_lr, n_estimators=50,\n",
        "                                   random_state=random_seed, n_jobs=-1)\n",
        "bagging_clf_lr.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions for AUC\n",
        "y_pred_proba = bagging_clf_lr.predict_proba(X_test)[:, 1] # Probability of positive class\n",
        "\n",
        "# Evaluate AUC\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Bagging Classifier (Logistic Regression base) AUC: {auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urtk8Ic02K74",
        "outputId": "7082137a-1f46-4ec8-e6e1-0006a7288c20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 8. Bagging Classifier with Logistic Regression Base (AUC) ---\n",
            "Bagging Classifier (Logistic Regression base) AUC: 0.9075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a Random Forest Regressor and analyze feature importance scores\n",
        "print(\"\\n--- 9. Random Forest Regressor & Feature Importance Analysis ---\")\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15,\n",
        "                       noise=0.1, random_state=random_seed)\n",
        "# Create dummy feature names for demonstration\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# Create and train the Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate MSE (optional)\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Random Forest Regressor MSE: {mse:.4f}\")\n",
        "\n",
        "# Get feature importances (Mean Decrease in Impurity/Variance Reduction)\n",
        "importances = rf_reg.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"\\nFeature Importance Scores (MDI/Variance Reduction):\")\n",
        "# Print top 10 features\n",
        "for i in range(min(10, X.shape[1])):\n",
        "    print(f\"{i + 1}. Feature '{feature_names[indices[i]]}' ({importances[indices[i]]:.4f})\")\n",
        "\n",
        "# Analysis: Features with higher scores contribute more, on average, to reducing\n",
        "# the variance (or MSE) at the splits in the trees across the forest.\n",
        "# In this synthetic dataset, we expect the informative features (first 15)\n",
        "# to generally have higher importance scores than the non-informative ones."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIrksR193L02",
        "outputId": "1286df7f-d8a3-4bd4-a03a-0b49b950466c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 9. Random Forest Regressor & Feature Importance Analysis ---\n",
            "Random Forest Regressor MSE: 17137.6945\n",
            "\n",
            "Feature Importance Scores (MDI/Variance Reduction):\n",
            "1. Feature 'feature_3' (0.2123)\n",
            "2. Feature 'feature_2' (0.1586)\n",
            "3. Feature 'feature_8' (0.0885)\n",
            "4. Feature 'feature_14' (0.0865)\n",
            "5. Feature 'feature_13' (0.0844)\n",
            "6. Feature 'feature_9' (0.0734)\n",
            "7. Feature 'feature_0' (0.0716)\n",
            "8. Feature 'feature_12' (0.0385)\n",
            "9. Feature 'feature_6' (0.0273)\n",
            "10. Feature 'feature_16' (0.0228)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "print(\"\\n--- 10. Comparing Bagging (DT) vs Random Forest Classifier Accuracy ---\")\n",
        "# Use Breast Cancer data\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# --- Bagging Classifier (with Decision Trees) ---\n",
        "base_dt = DecisionTreeClassifier(random_state=random_seed)\n",
        "bagging_clf = BaggingClassifier(estimator=base_dt, n_estimators=100, # Use same n_estimators for fair comparison\n",
        "                                random_state=random_seed, n_jobs=-1)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Bagging Classifier (DT base) Accuracy: {accuracy_bagging:.4f}\")\n",
        "\n",
        "# --- Random Forest Classifier ---\n",
        "# Note: RF is essentially Bagging(DT) + Feature Randomness\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "if accuracy_rf > accuracy_bagging:\n",
        "    print(\"Random Forest performed better.\")\n",
        "elif accuracy_bagging > accuracy_rf:\n",
        "     print(\"Bagging Classifier (DT base) performed better.\")\n",
        "else:\n",
        "    print(\"Both models performed equally well.\")\n",
        "\n",
        "# Typically, Random Forest often slightly outperforms a standard Bagging Classifier\n",
        "# with Decision Trees due to the added decorrelation from feature randomness,\n",
        "# especially when some features are very dominant."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoee6JZg3qrp",
        "outputId": "d3620ad8-6333-4310-98a2-9981e4d737da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 10. Comparing Bagging (DT) vs Random Forest Classifier Accuracy ---\n",
            "Bagging Classifier (DT base) Accuracy: 0.9591\n",
            "Random Forest Classifier Accuracy: 0.9708\n",
            "Random Forest performed better.\n"
          ]
        }
      ]
    }
  ]
}